{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/biplab/sarthak/miniconda3/envs/flsetup/lib/python3.9/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from model import MultiLoRAViT\n",
    "from dataloader import AnimeCharacterDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create dataset\n",
    "dataset = AnimeCharacterDataset(\n",
    "    csv_path='train_data.csv',\n",
    "    img_dir='single_characters'\n",
    ")\n",
    "test_dataset = AnimeCharacterDataset(\n",
    "    csv_path='test_data.csv',\n",
    "    img_dir='single_characters'\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "with open('encodings.json', 'r') as f:\n",
    "    encodings = json.load(f)\n",
    "\n",
    "adapter_config = {\n",
    "    'Age': len(encodings['Age']),\n",
    "    'Gender': len(encodings['Gender']),\n",
    "    'Ethnicity': len(encodings['Ethnicity']),\n",
    "    'Hair Style': len(encodings['Hair Style']),\n",
    "    'Hair Color': len(encodings['Hair Color']),\n",
    "    'Hair Length': len(encodings['Hair Length']),\n",
    "    'Eye Color': len(encodings['Eye Color']),\n",
    "    'Body Type': len(encodings['Body Type']),\n",
    "    'Dress': len(encodings['Dress'])\n",
    "}\n",
    "\n",
    "\n",
    "model = MultiLoRAViT(adapter_config, r=4)\n",
    "model.switch_adapter('Age')  # Switch to Age adapter\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizers = {\n",
    "    name: optim.Adam(\n",
    "        list(model.heads[name].parameters()) + \n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=1e-3\n",
    "    ) for name in adapter_config.keys()\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Adapter: Age, Batch: 0, Loss: 2.4940\n",
      "Epoch: 0, Adapter: Gender, Batch: 0, Loss: 1.8274\n",
      "Epoch: 0, Adapter: Ethnicity, Batch: 0, Loss: 2.3091\n",
      "Epoch: 0, Adapter: Hair Style, Batch: 0, Loss: 3.7287\n",
      "Epoch: 0, Adapter: Hair Color, Batch: 0, Loss: 3.5526\n",
      "Epoch: 0, Adapter: Hair Length, Batch: 0, Loss: 1.5644\n",
      "Epoch: 0, Adapter: Eye Color, Batch: 0, Loss: 3.5098\n",
      "Epoch: 0, Adapter: Body Type, Batch: 0, Loss: 3.4425\n",
      "Epoch: 0, Adapter: Dress, Batch: 0, Loss: 4.5570\n",
      "Epoch: 0, Adapter: Age, Batch: 10, Loss: 1.7198\n",
      "Epoch: 0, Adapter: Gender, Batch: 10, Loss: 0.9180\n",
      "Epoch: 0, Adapter: Ethnicity, Batch: 10, Loss: 1.2269\n",
      "Epoch: 0, Adapter: Hair Style, Batch: 10, Loss: 2.0251\n",
      "Epoch: 0, Adapter: Hair Color, Batch: 10, Loss: 2.0979\n",
      "Epoch: 0, Adapter: Hair Length, Batch: 10, Loss: 0.7122\n",
      "Epoch: 0, Adapter: Eye Color, Batch: 10, Loss: 2.2635\n",
      "Epoch: 0, Adapter: Body Type, Batch: 10, Loss: 1.9597\n",
      "Epoch: 0, Adapter: Dress, Batch: 10, Loss: 3.0974\n",
      "Epoch: 0, Adapter: Age, Batch: 20, Loss: 1.6025\n",
      "Epoch: 0, Adapter: Gender, Batch: 20, Loss: 0.3759\n",
      "Epoch: 0, Adapter: Ethnicity, Batch: 20, Loss: 1.1408\n",
      "Epoch: 0, Adapter: Hair Style, Batch: 20, Loss: 1.4593\n",
      "Epoch: 0, Adapter: Hair Color, Batch: 20, Loss: 1.6916\n",
      "Epoch: 0, Adapter: Hair Length, Batch: 20, Loss: 0.6463\n",
      "Epoch: 0, Adapter: Eye Color, Batch: 20, Loss: 2.0562\n",
      "Epoch: 0, Adapter: Body Type, Batch: 20, Loss: 1.8696\n",
      "Epoch: 0, Adapter: Dress, Batch: 20, Loss: 3.1069\n",
      "\n",
      "Epoch 0 Validation Results:\n",
      "Age: Accuracy: 49.77%, Avg Loss: 1.5220\n",
      "Saving model for adapter: Age\n",
      "Gender: Accuracy: 86.26%, Avg Loss: 0.3465\n",
      "Saving model for adapter: Gender\n",
      "Ethnicity: Accuracy: 56.76%, Avg Loss: 1.0754\n",
      "Saving model for adapter: Ethnicity\n",
      "Hair Style: Accuracy: 42.12%, Avg Loss: 1.7382\n",
      "Saving model for adapter: Hair Style\n",
      "Hair Color: Accuracy: 58.33%, Avg Loss: 1.3917\n",
      "Saving model for adapter: Hair Color\n",
      "Hair Length: Accuracy: 71.62%, Avg Loss: 0.7215\n",
      "Saving model for adapter: Hair Length\n",
      "Eye Color: Accuracy: 40.77%, Avg Loss: 1.9817\n",
      "Saving model for adapter: Eye Color\n",
      "Body Type: Accuracy: 37.61%, Avg Loss: 1.8873\n",
      "Saving model for adapter: Body Type\n",
      "Dress: Accuracy: 27.25%, Avg Loss: 2.8728\n",
      "Saving model for adapter: Dress\n",
      "\n",
      "Epoch: 1, Adapter: Age, Batch: 0, Loss: 1.4752\n",
      "Epoch: 1, Adapter: Gender, Batch: 0, Loss: 0.3229\n",
      "Epoch: 1, Adapter: Ethnicity, Batch: 0, Loss: 0.9137\n",
      "Epoch: 1, Adapter: Hair Style, Batch: 0, Loss: 1.4476\n",
      "Epoch: 1, Adapter: Hair Color, Batch: 0, Loss: 1.0326\n",
      "Epoch: 1, Adapter: Hair Length, Batch: 0, Loss: 0.7134\n",
      "Epoch: 1, Adapter: Eye Color, Batch: 0, Loss: 1.7353\n",
      "Epoch: 1, Adapter: Body Type, Batch: 0, Loss: 1.5712\n",
      "Epoch: 1, Adapter: Dress, Batch: 0, Loss: 2.3150\n",
      "Epoch: 1, Adapter: Age, Batch: 10, Loss: 1.1027\n",
      "Epoch: 1, Adapter: Gender, Batch: 10, Loss: 0.1481\n",
      "Epoch: 1, Adapter: Ethnicity, Batch: 10, Loss: 1.3611\n",
      "Epoch: 1, Adapter: Hair Style, Batch: 10, Loss: 1.2242\n",
      "Epoch: 1, Adapter: Hair Color, Batch: 10, Loss: 0.9687\n",
      "Epoch: 1, Adapter: Hair Length, Batch: 10, Loss: 0.5725\n",
      "Epoch: 1, Adapter: Eye Color, Batch: 10, Loss: 1.5701\n",
      "Epoch: 1, Adapter: Body Type, Batch: 10, Loss: 1.2812\n",
      "Epoch: 1, Adapter: Dress, Batch: 10, Loss: 1.9910\n",
      "Epoch: 1, Adapter: Age, Batch: 20, Loss: 1.1059\n",
      "Epoch: 1, Adapter: Gender, Batch: 20, Loss: 0.3024\n",
      "Epoch: 1, Adapter: Ethnicity, Batch: 20, Loss: 0.8292\n",
      "Epoch: 1, Adapter: Hair Style, Batch: 20, Loss: 1.3689\n",
      "Epoch: 1, Adapter: Hair Color, Batch: 20, Loss: 0.9495\n",
      "Epoch: 1, Adapter: Hair Length, Batch: 20, Loss: 0.5920\n",
      "Epoch: 1, Adapter: Eye Color, Batch: 20, Loss: 1.4778\n",
      "Epoch: 1, Adapter: Body Type, Batch: 20, Loss: 1.3736\n",
      "Epoch: 1, Adapter: Dress, Batch: 20, Loss: 1.9776\n",
      "\n",
      "Epoch 1 Validation Results:\n",
      "Age: Accuracy: 49.10%, Avg Loss: 1.4449\n",
      "Gender: Accuracy: 90.77%, Avg Loss: 0.2120\n",
      "Saving model for adapter: Gender\n",
      "Ethnicity: Accuracy: 63.51%, Avg Loss: 0.9505\n",
      "Saving model for adapter: Ethnicity\n",
      "Hair Style: Accuracy: 47.97%, Avg Loss: 1.5409\n",
      "Saving model for adapter: Hair Style\n",
      "Hair Color: Accuracy: 65.77%, Avg Loss: 1.1096\n",
      "Saving model for adapter: Hair Color\n",
      "Hair Length: Accuracy: 71.40%, Avg Loss: 0.6978\n",
      "Eye Color: Accuracy: 49.10%, Avg Loss: 1.7396\n",
      "Saving model for adapter: Eye Color\n",
      "Body Type: Accuracy: 38.96%, Avg Loss: 1.7432\n",
      "Saving model for adapter: Body Type\n",
      "Dress: Accuracy: 31.08%, Avg Loss: 2.6017\n",
      "Saving model for adapter: Dress\n",
      "\n",
      "Epoch: 2, Adapter: Age, Batch: 0, Loss: 1.0282\n",
      "Epoch: 2, Adapter: Gender, Batch: 0, Loss: 0.0779\n",
      "Epoch: 2, Adapter: Ethnicity, Batch: 0, Loss: 0.7898\n",
      "Epoch: 2, Adapter: Hair Style, Batch: 0, Loss: 1.1083\n",
      "Epoch: 2, Adapter: Hair Color, Batch: 0, Loss: 0.7021\n",
      "Epoch: 2, Adapter: Hair Length, Batch: 0, Loss: 0.4799\n",
      "Epoch: 2, Adapter: Eye Color, Batch: 0, Loss: 1.3406\n",
      "Epoch: 2, Adapter: Body Type, Batch: 0, Loss: 1.1253\n",
      "Epoch: 2, Adapter: Dress, Batch: 0, Loss: 1.6746\n",
      "Epoch: 2, Adapter: Age, Batch: 10, Loss: 1.0567\n",
      "Epoch: 2, Adapter: Gender, Batch: 10, Loss: 0.1132\n",
      "Epoch: 2, Adapter: Ethnicity, Batch: 10, Loss: 0.6362\n",
      "Epoch: 2, Adapter: Hair Style, Batch: 10, Loss: 1.1518\n",
      "Epoch: 2, Adapter: Hair Color, Batch: 10, Loss: 0.5564\n",
      "Epoch: 2, Adapter: Hair Length, Batch: 10, Loss: 0.4064\n",
      "Epoch: 2, Adapter: Eye Color, Batch: 10, Loss: 1.1865\n",
      "Epoch: 2, Adapter: Body Type, Batch: 10, Loss: 1.1339\n",
      "Epoch: 2, Adapter: Dress, Batch: 10, Loss: 1.6762\n",
      "Epoch: 2, Adapter: Age, Batch: 20, Loss: 1.1474\n",
      "Epoch: 2, Adapter: Gender, Batch: 20, Loss: 0.0736\n",
      "Epoch: 2, Adapter: Ethnicity, Batch: 20, Loss: 0.8010\n",
      "Epoch: 2, Adapter: Hair Style, Batch: 20, Loss: 1.2445\n",
      "Epoch: 2, Adapter: Hair Color, Batch: 20, Loss: 0.4006\n",
      "Epoch: 2, Adapter: Hair Length, Batch: 20, Loss: 0.4436\n",
      "Epoch: 2, Adapter: Eye Color, Batch: 20, Loss: 1.2289\n",
      "Epoch: 2, Adapter: Body Type, Batch: 20, Loss: 1.2461\n",
      "Epoch: 2, Adapter: Dress, Batch: 20, Loss: 1.6530\n",
      "\n",
      "Epoch 2 Validation Results:\n",
      "Age: Accuracy: 53.38%, Avg Loss: 1.4002\n",
      "Saving model for adapter: Age\n",
      "Gender: Accuracy: 93.02%, Avg Loss: 0.1497\n",
      "Saving model for adapter: Gender\n",
      "Ethnicity: Accuracy: 63.29%, Avg Loss: 0.9485\n",
      "Hair Style: Accuracy: 48.65%, Avg Loss: 1.5553\n",
      "Saving model for adapter: Hair Style\n",
      "Hair Color: Accuracy: 68.47%, Avg Loss: 1.0217\n",
      "Saving model for adapter: Hair Color\n",
      "Hair Length: Accuracy: 75.68%, Avg Loss: 0.6652\n",
      "Saving model for adapter: Hair Length\n",
      "Eye Color: Accuracy: 53.38%, Avg Loss: 1.6915\n",
      "Saving model for adapter: Eye Color\n",
      "Body Type: Accuracy: 38.51%, Avg Loss: 1.7401\n",
      "Dress: Accuracy: 34.01%, Avg Loss: 2.4921\n",
      "Saving model for adapter: Dress\n",
      "\n",
      "Epoch: 3, Adapter: Age, Batch: 0, Loss: 0.9273\n",
      "Epoch: 3, Adapter: Gender, Batch: 0, Loss: 0.0283\n",
      "Epoch: 3, Adapter: Ethnicity, Batch: 0, Loss: 0.4655\n",
      "Epoch: 3, Adapter: Hair Style, Batch: 0, Loss: 0.7966\n",
      "Epoch: 3, Adapter: Hair Color, Batch: 0, Loss: 0.3944\n",
      "Epoch: 3, Adapter: Hair Length, Batch: 0, Loss: 0.3097\n",
      "Epoch: 3, Adapter: Eye Color, Batch: 0, Loss: 0.8642\n",
      "Epoch: 3, Adapter: Body Type, Batch: 0, Loss: 1.0348\n",
      "Epoch: 3, Adapter: Dress, Batch: 0, Loss: 1.0247\n",
      "Epoch: 3, Adapter: Age, Batch: 10, Loss: 0.8290\n",
      "Epoch: 3, Adapter: Gender, Batch: 10, Loss: 0.0385\n",
      "Epoch: 3, Adapter: Ethnicity, Batch: 10, Loss: 0.5703\n",
      "Epoch: 3, Adapter: Hair Style, Batch: 10, Loss: 0.8137\n",
      "Epoch: 3, Adapter: Hair Color, Batch: 10, Loss: 0.5691\n",
      "Epoch: 3, Adapter: Hair Length, Batch: 10, Loss: 0.5027\n",
      "Epoch: 3, Adapter: Eye Color, Batch: 10, Loss: 0.7896\n",
      "Epoch: 3, Adapter: Body Type, Batch: 10, Loss: 1.2277\n",
      "Epoch: 3, Adapter: Dress, Batch: 10, Loss: 0.8362\n",
      "Epoch: 3, Adapter: Age, Batch: 20, Loss: 1.1242\n",
      "Epoch: 3, Adapter: Gender, Batch: 20, Loss: 0.0308\n",
      "Epoch: 3, Adapter: Ethnicity, Batch: 20, Loss: 0.6441\n",
      "Epoch: 3, Adapter: Hair Style, Batch: 20, Loss: 0.8591\n",
      "Epoch: 3, Adapter: Hair Color, Batch: 20, Loss: 0.5469\n",
      "Epoch: 3, Adapter: Hair Length, Batch: 20, Loss: 0.3690\n",
      "Epoch: 3, Adapter: Eye Color, Batch: 20, Loss: 0.9723\n",
      "Epoch: 3, Adapter: Body Type, Batch: 20, Loss: 1.0395\n",
      "Epoch: 3, Adapter: Dress, Batch: 20, Loss: 1.2059\n",
      "\n",
      "Epoch 3 Validation Results:\n",
      "Age: Accuracy: 52.48%, Avg Loss: 1.4314\n",
      "Gender: Accuracy: 94.59%, Avg Loss: 0.1197\n",
      "Saving model for adapter: Gender\n",
      "Ethnicity: Accuracy: 63.51%, Avg Loss: 0.9149\n",
      "Hair Style: Accuracy: 48.87%, Avg Loss: 1.5387\n",
      "Saving model for adapter: Hair Style\n",
      "Hair Color: Accuracy: 70.50%, Avg Loss: 0.9843\n",
      "Saving model for adapter: Hair Color\n",
      "Hair Length: Accuracy: 77.48%, Avg Loss: 0.6787\n",
      "Saving model for adapter: Hair Length\n",
      "Eye Color: Accuracy: 53.38%, Avg Loss: 1.6900\n",
      "Body Type: Accuracy: 34.23%, Avg Loss: 1.8257\n",
      "Dress: Accuracy: 34.68%, Avg Loss: 2.5382\n",
      "Saving model for adapter: Dress\n",
      "\n",
      "Epoch: 4, Adapter: Age, Batch: 0, Loss: 0.6549\n",
      "Epoch: 4, Adapter: Gender, Batch: 0, Loss: 0.0418\n",
      "Epoch: 4, Adapter: Ethnicity, Batch: 0, Loss: 0.4083\n",
      "Epoch: 4, Adapter: Hair Style, Batch: 0, Loss: 0.5746\n",
      "Epoch: 4, Adapter: Hair Color, Batch: 0, Loss: 0.3624\n",
      "Epoch: 4, Adapter: Hair Length, Batch: 0, Loss: 0.3312\n",
      "Epoch: 4, Adapter: Eye Color, Batch: 0, Loss: 0.7674\n",
      "Epoch: 4, Adapter: Body Type, Batch: 0, Loss: 0.8266\n",
      "Epoch: 4, Adapter: Dress, Batch: 0, Loss: 0.7159\n",
      "Epoch: 4, Adapter: Age, Batch: 10, Loss: 0.7521\n",
      "Epoch: 4, Adapter: Gender, Batch: 10, Loss: 0.0189\n",
      "Epoch: 4, Adapter: Ethnicity, Batch: 10, Loss: 0.4345\n",
      "Epoch: 4, Adapter: Hair Style, Batch: 10, Loss: 0.4369\n",
      "Epoch: 4, Adapter: Hair Color, Batch: 10, Loss: 0.3756\n",
      "Epoch: 4, Adapter: Hair Length, Batch: 10, Loss: 0.1964\n",
      "Epoch: 4, Adapter: Eye Color, Batch: 10, Loss: 0.7989\n",
      "Epoch: 4, Adapter: Body Type, Batch: 10, Loss: 0.9552\n",
      "Epoch: 4, Adapter: Dress, Batch: 10, Loss: 0.7671\n",
      "Epoch: 4, Adapter: Age, Batch: 20, Loss: 0.7095\n",
      "Epoch: 4, Adapter: Gender, Batch: 20, Loss: 0.0172\n",
      "Epoch: 4, Adapter: Ethnicity, Batch: 20, Loss: 0.5325\n",
      "Epoch: 4, Adapter: Hair Style, Batch: 20, Loss: 0.4996\n",
      "Epoch: 4, Adapter: Hair Color, Batch: 20, Loss: 0.3858\n",
      "Epoch: 4, Adapter: Hair Length, Batch: 20, Loss: 0.3545\n",
      "Epoch: 4, Adapter: Eye Color, Batch: 20, Loss: 0.8276\n",
      "Epoch: 4, Adapter: Body Type, Batch: 20, Loss: 0.9843\n",
      "Epoch: 4, Adapter: Dress, Batch: 20, Loss: 0.6862\n",
      "\n",
      "Epoch 4 Validation Results:\n",
      "Age: Accuracy: 54.50%, Avg Loss: 1.4326\n",
      "Saving model for adapter: Age\n",
      "Gender: Accuracy: 95.72%, Avg Loss: 0.1193\n",
      "Saving model for adapter: Gender\n",
      "Ethnicity: Accuracy: 62.16%, Avg Loss: 0.9561\n",
      "Hair Style: Accuracy: 47.75%, Avg Loss: 1.5898\n",
      "Hair Color: Accuracy: 72.07%, Avg Loss: 0.9890\n",
      "Saving model for adapter: Hair Color\n",
      "Hair Length: Accuracy: 76.35%, Avg Loss: 0.7327\n",
      "Eye Color: Accuracy: 55.86%, Avg Loss: 1.7538\n",
      "Saving model for adapter: Eye Color\n",
      "Body Type: Accuracy: 37.61%, Avg Loss: 1.9243\n",
      "Dress: Accuracy: 35.59%, Avg Loss: 2.6445\n",
      "Saving model for adapter: Dress\n",
      "\n",
      "Epoch: 5, Adapter: Age, Batch: 0, Loss: 0.5591\n",
      "Epoch: 5, Adapter: Gender, Batch: 0, Loss: 0.0190\n",
      "Epoch: 5, Adapter: Ethnicity, Batch: 0, Loss: 0.3937\n",
      "Epoch: 5, Adapter: Hair Style, Batch: 0, Loss: 0.5946\n",
      "Epoch: 5, Adapter: Hair Color, Batch: 0, Loss: 0.2886\n",
      "Epoch: 5, Adapter: Hair Length, Batch: 0, Loss: 0.2655\n",
      "Epoch: 5, Adapter: Eye Color, Batch: 0, Loss: 0.5474\n",
      "Epoch: 5, Adapter: Body Type, Batch: 0, Loss: 0.8489\n",
      "Epoch: 5, Adapter: Dress, Batch: 0, Loss: 0.4782\n",
      "Epoch: 5, Adapter: Age, Batch: 10, Loss: 0.6530\n",
      "Epoch: 5, Adapter: Gender, Batch: 10, Loss: 0.0116\n",
      "Epoch: 5, Adapter: Ethnicity, Batch: 10, Loss: 0.3023\n",
      "Epoch: 5, Adapter: Hair Style, Batch: 10, Loss: 0.5568\n",
      "Epoch: 5, Adapter: Hair Color, Batch: 10, Loss: 0.2027\n",
      "Epoch: 5, Adapter: Hair Length, Batch: 10, Loss: 0.1595\n",
      "Epoch: 5, Adapter: Eye Color, Batch: 10, Loss: 0.5166\n",
      "Epoch: 5, Adapter: Body Type, Batch: 10, Loss: 0.8564\n",
      "Epoch: 5, Adapter: Dress, Batch: 10, Loss: 0.4963\n",
      "Epoch: 5, Adapter: Age, Batch: 20, Loss: 0.6557\n",
      "Epoch: 5, Adapter: Gender, Batch: 20, Loss: 0.0098\n",
      "Epoch: 5, Adapter: Ethnicity, Batch: 20, Loss: 0.2860\n",
      "Epoch: 5, Adapter: Hair Style, Batch: 20, Loss: 0.5384\n",
      "Epoch: 5, Adapter: Hair Color, Batch: 20, Loss: 0.1993\n",
      "Epoch: 5, Adapter: Hair Length, Batch: 20, Loss: 0.2276\n",
      "Epoch: 5, Adapter: Eye Color, Batch: 20, Loss: 0.5330\n",
      "Epoch: 5, Adapter: Body Type, Batch: 20, Loss: 0.6451\n",
      "Epoch: 5, Adapter: Dress, Batch: 20, Loss: 0.4440\n",
      "\n",
      "Epoch 5 Validation Results:\n",
      "Age: Accuracy: 54.50%, Avg Loss: 1.4751\n",
      "Gender: Accuracy: 95.72%, Avg Loss: 0.1276\n",
      "Ethnicity: Accuracy: 64.19%, Avg Loss: 1.0205\n",
      "Saving model for adapter: Ethnicity\n",
      "Hair Style: Accuracy: 50.68%, Avg Loss: 1.6582\n",
      "Saving model for adapter: Hair Style\n",
      "Hair Color: Accuracy: 69.82%, Avg Loss: 1.0268\n",
      "Hair Length: Accuracy: 78.60%, Avg Loss: 0.7295\n",
      "Saving model for adapter: Hair Length\n",
      "Eye Color: Accuracy: 54.95%, Avg Loss: 1.8219\n",
      "Body Type: Accuracy: 35.81%, Avg Loss: 2.0095\n",
      "Dress: Accuracy: 35.81%, Avg Loss: 2.7551\n",
      "Saving model for adapter: Dress\n",
      "\n",
      "Epoch: 6, Adapter: Age, Batch: 0, Loss: 0.3817\n",
      "Epoch: 6, Adapter: Gender, Batch: 0, Loss: 0.0034\n",
      "Epoch: 6, Adapter: Ethnicity, Batch: 0, Loss: 0.1935\n",
      "Epoch: 6, Adapter: Hair Style, Batch: 0, Loss: 0.3654\n",
      "Epoch: 6, Adapter: Hair Color, Batch: 0, Loss: 0.1625\n",
      "Epoch: 6, Adapter: Hair Length, Batch: 0, Loss: 0.0595\n",
      "Epoch: 6, Adapter: Eye Color, Batch: 0, Loss: 0.3098\n",
      "Epoch: 6, Adapter: Body Type, Batch: 0, Loss: 0.3972\n",
      "Epoch: 6, Adapter: Dress, Batch: 0, Loss: 0.4033\n",
      "Epoch: 6, Adapter: Age, Batch: 10, Loss: 0.4250\n",
      "Epoch: 6, Adapter: Gender, Batch: 10, Loss: 0.0058\n",
      "Epoch: 6, Adapter: Ethnicity, Batch: 10, Loss: 0.2679\n",
      "Epoch: 6, Adapter: Hair Style, Batch: 10, Loss: 0.3433\n",
      "Epoch: 6, Adapter: Hair Color, Batch: 10, Loss: 0.2150\n",
      "Epoch: 6, Adapter: Hair Length, Batch: 10, Loss: 0.0645\n",
      "Epoch: 6, Adapter: Eye Color, Batch: 10, Loss: 0.4084\n",
      "Epoch: 6, Adapter: Body Type, Batch: 10, Loss: 0.4690\n",
      "Epoch: 6, Adapter: Dress, Batch: 10, Loss: 0.3146\n",
      "Epoch: 6, Adapter: Age, Batch: 20, Loss: 0.3650\n",
      "Epoch: 6, Adapter: Gender, Batch: 20, Loss: 0.0027\n",
      "Epoch: 6, Adapter: Ethnicity, Batch: 20, Loss: 0.2056\n",
      "Epoch: 6, Adapter: Hair Style, Batch: 20, Loss: 0.3131\n",
      "Epoch: 6, Adapter: Hair Color, Batch: 20, Loss: 0.0797\n",
      "Epoch: 6, Adapter: Hair Length, Batch: 20, Loss: 0.0515\n",
      "Epoch: 6, Adapter: Eye Color, Batch: 20, Loss: 0.3570\n",
      "Epoch: 6, Adapter: Body Type, Batch: 20, Loss: 0.6075\n",
      "Epoch: 6, Adapter: Dress, Batch: 20, Loss: 0.3182\n",
      "\n",
      "Epoch 6 Validation Results:\n",
      "Age: Accuracy: 51.80%, Avg Loss: 1.5458\n",
      "Gender: Accuracy: 95.95%, Avg Loss: 0.1258\n",
      "Saving model for adapter: Gender\n",
      "Ethnicity: Accuracy: 64.19%, Avg Loss: 1.1152\n",
      "Hair Style: Accuracy: 49.77%, Avg Loss: 1.7513\n",
      "Hair Color: Accuracy: 72.97%, Avg Loss: 1.0389\n",
      "Saving model for adapter: Hair Color\n",
      "Hair Length: Accuracy: 77.70%, Avg Loss: 0.7854\n",
      "Eye Color: Accuracy: 54.95%, Avg Loss: 1.9032\n",
      "Body Type: Accuracy: 34.23%, Avg Loss: 2.2015\n",
      "Dress: Accuracy: 37.84%, Avg Loss: 2.8281\n",
      "Saving model for adapter: Dress\n",
      "\n",
      "Epoch: 7, Adapter: Age, Batch: 0, Loss: 0.3045\n",
      "Epoch: 7, Adapter: Gender, Batch: 0, Loss: 0.0020\n",
      "Epoch: 7, Adapter: Ethnicity, Batch: 0, Loss: 0.1670\n",
      "Epoch: 7, Adapter: Hair Style, Batch: 0, Loss: 0.2030\n",
      "Epoch: 7, Adapter: Hair Color, Batch: 0, Loss: 0.0778\n",
      "Epoch: 7, Adapter: Hair Length, Batch: 0, Loss: 0.0397\n",
      "Epoch: 7, Adapter: Eye Color, Batch: 0, Loss: 0.2057\n",
      "Epoch: 7, Adapter: Body Type, Batch: 0, Loss: 0.4357\n",
      "Epoch: 7, Adapter: Dress, Batch: 0, Loss: 0.1806\n",
      "Epoch: 7, Adapter: Age, Batch: 10, Loss: 0.1762\n",
      "Epoch: 7, Adapter: Gender, Batch: 10, Loss: 0.0025\n",
      "Epoch: 7, Adapter: Ethnicity, Batch: 10, Loss: 0.0788\n",
      "Epoch: 7, Adapter: Hair Style, Batch: 10, Loss: 0.1148\n",
      "Epoch: 7, Adapter: Hair Color, Batch: 10, Loss: 0.0844\n",
      "Epoch: 7, Adapter: Hair Length, Batch: 10, Loss: 0.0488\n",
      "Epoch: 7, Adapter: Eye Color, Batch: 10, Loss: 0.1513\n",
      "Epoch: 7, Adapter: Body Type, Batch: 10, Loss: 0.3966\n",
      "Epoch: 7, Adapter: Dress, Batch: 10, Loss: 0.2005\n",
      "Epoch: 7, Adapter: Age, Batch: 20, Loss: 0.2813\n",
      "Epoch: 7, Adapter: Gender, Batch: 20, Loss: 0.0017\n",
      "Epoch: 7, Adapter: Ethnicity, Batch: 20, Loss: 0.1660\n",
      "Epoch: 7, Adapter: Hair Style, Batch: 20, Loss: 0.2146\n",
      "Epoch: 7, Adapter: Hair Color, Batch: 20, Loss: 0.0791\n",
      "Epoch: 7, Adapter: Hair Length, Batch: 20, Loss: 0.0428\n",
      "Epoch: 7, Adapter: Eye Color, Batch: 20, Loss: 0.2543\n",
      "Epoch: 7, Adapter: Body Type, Batch: 20, Loss: 0.4490\n",
      "Epoch: 7, Adapter: Dress, Batch: 20, Loss: 0.1838\n",
      "\n",
      "Epoch 7 Validation Results:\n",
      "Age: Accuracy: 50.68%, Avg Loss: 1.6672\n",
      "Gender: Accuracy: 95.72%, Avg Loss: 0.1431\n",
      "Ethnicity: Accuracy: 64.19%, Avg Loss: 1.2178\n",
      "Hair Style: Accuracy: 46.62%, Avg Loss: 1.8734\n",
      "Hair Color: Accuracy: 70.50%, Avg Loss: 1.1144\n",
      "Hair Length: Accuracy: 77.70%, Avg Loss: 0.8377\n",
      "Eye Color: Accuracy: 53.60%, Avg Loss: 2.0017\n",
      "Body Type: Accuracy: 36.71%, Avg Loss: 2.3409\n",
      "Dress: Accuracy: 35.81%, Avg Loss: 2.9318\n",
      "\n",
      "Epoch: 8, Adapter: Age, Batch: 0, Loss: 0.1847\n",
      "Epoch: 8, Adapter: Gender, Batch: 0, Loss: 0.0015\n",
      "Epoch: 8, Adapter: Ethnicity, Batch: 0, Loss: 0.0685\n",
      "Epoch: 8, Adapter: Hair Style, Batch: 0, Loss: 0.1199\n",
      "Epoch: 8, Adapter: Hair Color, Batch: 0, Loss: 0.0577\n",
      "Epoch: 8, Adapter: Hair Length, Batch: 0, Loss: 0.0117\n",
      "Epoch: 8, Adapter: Eye Color, Batch: 0, Loss: 0.1508\n",
      "Epoch: 8, Adapter: Body Type, Batch: 0, Loss: 0.2097\n",
      "Epoch: 8, Adapter: Dress, Batch: 0, Loss: 0.1124\n",
      "Epoch: 8, Adapter: Age, Batch: 10, Loss: 0.1113\n",
      "Epoch: 8, Adapter: Gender, Batch: 10, Loss: 0.0007\n",
      "Epoch: 8, Adapter: Ethnicity, Batch: 10, Loss: 0.0706\n",
      "Epoch: 8, Adapter: Hair Style, Batch: 10, Loss: 0.1350\n",
      "Epoch: 8, Adapter: Hair Color, Batch: 10, Loss: 0.0839\n",
      "Epoch: 8, Adapter: Hair Length, Batch: 10, Loss: 0.0195\n",
      "Epoch: 8, Adapter: Eye Color, Batch: 10, Loss: 0.1424\n",
      "Epoch: 8, Adapter: Body Type, Batch: 10, Loss: 0.1869\n",
      "Epoch: 8, Adapter: Dress, Batch: 10, Loss: 0.1098\n",
      "Epoch: 8, Adapter: Age, Batch: 20, Loss: 0.1007\n",
      "Epoch: 8, Adapter: Gender, Batch: 20, Loss: 0.0010\n",
      "Epoch: 8, Adapter: Ethnicity, Batch: 20, Loss: 0.0554\n",
      "Epoch: 8, Adapter: Hair Style, Batch: 20, Loss: 0.1161\n",
      "Epoch: 8, Adapter: Hair Color, Batch: 20, Loss: 0.0398\n",
      "Epoch: 8, Adapter: Hair Length, Batch: 20, Loss: 0.0290\n",
      "Epoch: 8, Adapter: Eye Color, Batch: 20, Loss: 0.0933\n",
      "Epoch: 8, Adapter: Body Type, Batch: 20, Loss: 0.1745\n",
      "Epoch: 8, Adapter: Dress, Batch: 20, Loss: 0.1257\n",
      "\n",
      "Epoch 8 Validation Results:\n",
      "Age: Accuracy: 54.05%, Avg Loss: 1.7333\n",
      "Gender: Accuracy: 95.50%, Avg Loss: 0.1418\n",
      "Ethnicity: Accuracy: 64.64%, Avg Loss: 1.3287\n",
      "Saving model for adapter: Ethnicity\n",
      "Hair Style: Accuracy: 50.23%, Avg Loss: 1.9921\n",
      "Hair Color: Accuracy: 73.42%, Avg Loss: 1.1075\n",
      "Saving model for adapter: Hair Color\n",
      "Hair Length: Accuracy: 78.15%, Avg Loss: 0.9008\n",
      "Eye Color: Accuracy: 53.60%, Avg Loss: 2.0314\n",
      "Body Type: Accuracy: 36.71%, Avg Loss: 2.5274\n",
      "Dress: Accuracy: 36.49%, Avg Loss: 3.0169\n",
      "\n",
      "Epoch: 9, Adapter: Age, Batch: 0, Loss: 0.0715\n",
      "Epoch: 9, Adapter: Gender, Batch: 0, Loss: 0.0008\n",
      "Epoch: 9, Adapter: Ethnicity, Batch: 0, Loss: 0.0236\n",
      "Epoch: 9, Adapter: Hair Style, Batch: 0, Loss: 0.0707\n",
      "Epoch: 9, Adapter: Hair Color, Batch: 0, Loss: 0.0243\n",
      "Epoch: 9, Adapter: Hair Length, Batch: 0, Loss: 0.0090\n",
      "Epoch: 9, Adapter: Eye Color, Batch: 0, Loss: 0.0670\n",
      "Epoch: 9, Adapter: Body Type, Batch: 0, Loss: 0.1443\n",
      "Epoch: 9, Adapter: Dress, Batch: 0, Loss: 0.0767\n",
      "Epoch: 9, Adapter: Age, Batch: 10, Loss: 0.1054\n",
      "Epoch: 9, Adapter: Gender, Batch: 10, Loss: 0.0007\n",
      "Epoch: 9, Adapter: Ethnicity, Batch: 10, Loss: 0.0301\n",
      "Epoch: 9, Adapter: Hair Style, Batch: 10, Loss: 0.0481\n",
      "Epoch: 9, Adapter: Hair Color, Batch: 10, Loss: 0.0279\n",
      "Epoch: 9, Adapter: Hair Length, Batch: 10, Loss: 0.0127\n",
      "Epoch: 9, Adapter: Eye Color, Batch: 10, Loss: 0.0704\n",
      "Epoch: 9, Adapter: Body Type, Batch: 10, Loss: 0.1570\n",
      "Epoch: 9, Adapter: Dress, Batch: 10, Loss: 0.0779\n",
      "Epoch: 9, Adapter: Age, Batch: 20, Loss: 0.0862\n",
      "Epoch: 9, Adapter: Gender, Batch: 20, Loss: 0.0011\n",
      "Epoch: 9, Adapter: Ethnicity, Batch: 20, Loss: 0.0312\n",
      "Epoch: 9, Adapter: Hair Style, Batch: 20, Loss: 0.0587\n",
      "Epoch: 9, Adapter: Hair Color, Batch: 20, Loss: 0.0251\n",
      "Epoch: 9, Adapter: Hair Length, Batch: 20, Loss: 0.0107\n",
      "Epoch: 9, Adapter: Eye Color, Batch: 20, Loss: 0.0720\n",
      "Epoch: 9, Adapter: Body Type, Batch: 20, Loss: 0.0966\n",
      "Epoch: 9, Adapter: Dress, Batch: 20, Loss: 0.0717\n",
      "\n",
      "Epoch 9 Validation Results:\n",
      "Age: Accuracy: 52.25%, Avg Loss: 1.8101\n",
      "Gender: Accuracy: 95.50%, Avg Loss: 0.1419\n",
      "Ethnicity: Accuracy: 65.54%, Avg Loss: 1.3990\n",
      "Saving model for adapter: Ethnicity\n",
      "Hair Style: Accuracy: 49.32%, Avg Loss: 2.0645\n",
      "Hair Color: Accuracy: 71.85%, Avg Loss: 1.1713\n",
      "Hair Length: Accuracy: 78.15%, Avg Loss: 0.9136\n",
      "Eye Color: Accuracy: 54.50%, Avg Loss: 2.1027\n",
      "Body Type: Accuracy: 38.51%, Avg Loss: 2.7018\n",
      "Dress: Accuracy: 36.49%, Avg Loss: 3.1051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_accuracies = {name: 0.0 for name in adapter_config.keys()}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Train each adapter separately\n",
    "        for adapter_idx, (adapter_name, num_classes) in enumerate(adapter_config.items()):\n",
    "            model.switch_adapter(adapter_name)\n",
    "            optimizer = optimizers[adapter_name]\n",
    "            \n",
    "            # Get corresponding label for this adapter\n",
    "            label = labels[:, adapter_idx].long().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, label)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch: {epoch}, Adapter: {adapter_name}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = {name: 0.0 for name in adapter_config.keys()}\n",
    "        val_correct = {name: 0 for name in adapter_config.keys()}\n",
    "        val_total = 0\n",
    "        \n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            val_total += images.size(0)\n",
    "            \n",
    "            for adapter_idx, (adapter_name, num_classes) in enumerate(adapter_config.items()):\n",
    "                model.switch_adapter(adapter_name)\n",
    "                label = labels[:, adapter_idx].long().to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, label)\n",
    "                val_losses[adapter_name] += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct[adapter_name] += predicted.eq(label).sum().item()\n",
    "        \n",
    "        # Print validation results\n",
    "        print(f'\\nEpoch {epoch} Validation Results:')\n",
    "        for adapter_name in adapter_config.keys():\n",
    "            acc = 100. * val_correct[adapter_name] / val_total\n",
    "            avg_loss = val_losses[adapter_name] / len(val_loader)\n",
    "            print(f'{adapter_name}: Accuracy: {acc:.2f}%, Avg Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            # Save the best model for each adapter based on accuracy\n",
    "            if acc > best_val_accuracies[adapter_name]:\n",
    "                best_val_accuracies[adapter_name] = acc\n",
    "                print(f'Saving model for adapter: {adapter_name}')\n",
    "                model.save_model(epoch=epoch, exp=f'checkpoints', adapter_name=adapter_name)\n",
    "                \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoints from epochs: {'Age': 4, 'Gender': 6, 'Ethnicity': 9, 'Hair Style': 5, 'Hair Color': 8, 'Hair Length': 5, 'Eye Color': 4, 'Body Type': 1, 'Dress': 6}\n",
      "\n",
      "Test Results:\n",
      "Age: Test Accuracy = 56.50%\n",
      "Gender: Test Accuracy = 95.53%\n",
      "Ethnicity: Test Accuracy = 57.32%\n",
      "Hair Style: Test Accuracy = 50.41%\n",
      "Hair Color: Test Accuracy = 73.17%\n",
      "Hair Length: Test Accuracy = 72.76%\n",
      "Eye Color: Test Accuracy = 51.63%\n",
      "Body Type: Test Accuracy = 43.50%\n",
      "Dress: Test Accuracy = 37.80%\n"
     ]
    }
   ],
   "source": [
    "from model import MultiLoRAViT\n",
    "from dataloader import AnimeCharacterDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "with open('encodings.json', 'r') as f:\n",
    "    encodings = json.load(f)\n",
    "\n",
    "adapter_config = {\n",
    "    'Age': len(encodings['Age']),\n",
    "    'Gender': len(encodings['Gender']),\n",
    "    'Ethnicity': len(encodings['Ethnicity']),\n",
    "    'Hair Style': len(encodings['Hair Style']),\n",
    "    'Hair Color': len(encodings['Hair Color']),\n",
    "    'Hair Length': len(encodings['Hair Length']),\n",
    "    'Eye Color': len(encodings['Eye Color']),\n",
    "    'Body Type': len(encodings['Body Type']),\n",
    "    'Dress': len(encodings['Dress'])\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiLoRAViT(adapter_config, r=4)\n",
    "\n",
    "epochs = model.load_model(exp='checkpoints')\n",
    "model = model.to(device)\n",
    "print(\"Loaded checkpoints from epochs:\", \n",
    "      {name: epoch for name, epoch in zip(adapter_config.keys(), epochs)})\n",
    "\n",
    "test_dataset = AnimeCharacterDataset(\n",
    "    csv_path='test_data.csv',\n",
    "    img_dir='single_characters'\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "results = {name: {'correct': 0, 'total': 0} for name in adapter_config.keys()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        for adapter_idx, (adapter_name, num_classes) in enumerate(adapter_config.items()):\n",
    "            model.switch_adapter(adapter_name)\n",
    "            label = labels[:, adapter_idx].long().to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            results[adapter_name]['correct'] += predicted.eq(label).sum().item()\n",
    "            results[adapter_name]['total'] += labels.size(0)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for adapter_name, metrics in results.items():\n",
    "    accuracy = 100. * metrics['correct'] / metrics['total']\n",
    "    print(f\"{adapter_name}: Test Accuracy = {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoints from epochs: {'Age': 4, 'Gender': 6, 'Ethnicity': 9, 'Hair Style': 5, 'Hair Color': 8, 'Hair Length': 5, 'Eye Color': 4, 'Body Type': 1, 'Dress': 6}\n"
     ]
    }
   ],
   "source": [
    "from model import MultiLoRAViT\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\n",
    "with open('encodings.json', 'r') as f:\n",
    "    encodings = json.load(f)\n",
    "\n",
    "adapter_config = {\n",
    "    'Age': len(encodings['Age']),\n",
    "    'Gender': len(encodings['Gender']),\n",
    "    'Ethnicity': len(encodings['Ethnicity']),\n",
    "    'Hair Style': len(encodings['Hair Style']),\n",
    "    'Hair Color': len(encodings['Hair Color']),\n",
    "    'Hair Length': len(encodings['Hair Length']),\n",
    "    'Eye Color': len(encodings['Eye Color']),\n",
    "    'Body Type': len(encodings['Body Type']),\n",
    "    'Dress': len(encodings['Dress'])\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiLoRAViT(adapter_config, r=4)\n",
    "\n",
    "epochs = model.load_model(exp='checkpoints')\n",
    "model = model.to(device)\n",
    "print(\"Loaded checkpoints from epochs:\", \n",
    "      {name: epoch for name, epoch in zip(adapter_config.keys(), epochs)})\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def predict_attributes(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        predictions = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for adapter_name in encodings.keys():\n",
    "                model.switch_adapter(adapter_name)\n",
    "                output = model(image_tensor)\n",
    "                pred_idx = output.argmax(1).item()\n",
    "                \n",
    "                reverse_encoding = {v: k for k, v in encodings[adapter_name].items()}\n",
    "                predictions[adapter_name] = reverse_encoding[pred_idx]\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: danbooru_431031_d94bbf167e4067caab3c36804f079f4d.jpg, Predicted attributes: {'Age': 'Young', 'Body Type': 'Unknown', 'Dress': 'Collared Shirt', 'Ethnicity': 'Japanese', 'Eye Color': 'Grey', 'Gender': 'Male', 'Hair Color': 'Black', 'Hair Length': 'Short', 'Hair Style': 'Short'}\n",
      "Image: danbooru_469064_98d34d4e476870f70b899274dc8e6547.jpg, Predicted attributes: {'Age': 'Early', 'Body Type': 'Normal', 'Dress': 'School Uniform', 'Ethnicity': 'Japanese', 'Eye Color': 'Brown', 'Gender': 'Female', 'Hair Color': 'Black', 'Hair Length': 'Long', 'Hair Style': 'Long'}\n",
      "Image: danbooru_469135_0355266341d00f6fed83a6081942ecad.jpg, Predicted attributes: {'Age': 'Young', 'Body Type': 'Slim', 'Dress': 'Maid', 'Ethnicity': 'Asian', 'Eye Color': 'Green', 'Gender': 'Male', 'Hair Color': 'Black', 'Hair Length': 'Long', 'Hair Style': 'Straight'}\n",
      "Image: danbooru_469090_9584201f870356e0bc04a44693408a1a.jpg, Predicted attributes: {'Age': 'Teen', 'Body Type': 'Slim', 'Dress': 'Serafuku', 'Ethnicity': 'Japanese', 'Eye Color': 'Not specified', 'Gender': 'Female', 'Hair Color': 'Black', 'Hair Length': 'Medium', 'Hair Style': 'Straight'}\n",
      "Image: danbooru_409987_522d28e8337a171803cfd5b0a3990d19.jpg, Predicted attributes: {'Age': 'Mid', 'Body Type': 'Slim', 'Dress': 'School Uniform', 'Ethnicity': 'Japanese', 'Eye Color': 'Blue', 'Gender': 'Female', 'Hair Color': 'Brown', 'Hair Length': 'Long', 'Hair Style': 'Long'}\n",
      "Image: danbooru_398821_525b329a700db7a59c3c234a7fb91655.jpg, Predicted attributes: {'Age': 'Young', 'Body Type': 'Slim', 'Dress': 'Serafuku', 'Ethnicity': 'Japanese', 'Eye Color': 'Blue', 'Gender': 'Female', 'Hair Color': 'Blonde', 'Hair Length': 'Long', 'Hair Style': 'Straight'}\n"
     ]
    }
   ],
   "source": [
    "# image_path = \"/raid/biplab/sarthak/dashtoon/single_characters/danbooru_398821_525b329a700db7a59c3c234a7fb91655.jpg\"\n",
    "import os\n",
    "for image in os.listdir('test_images'):\n",
    "    predictions = predict_attributes(f'test_images/{image}')\n",
    "    print(f\"Image: {image}, Predicted attributes: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flsetup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
